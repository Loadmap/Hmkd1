{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyNVG3hEJ9TmNOp3N81Or9Ap"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AyE6Cxigaoaw","executionInfo":{"status":"ok","timestamp":1689725837379,"user_tz":-540,"elapsed":122518,"user":{"displayName":"므름표","userId":"11964526023051922071"}},"outputId":"0bfe1208-566d-4e74-f571-229cc9b1bfec"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 26421880/26421880 [00:03<00:00, 8456932.08it/s] \n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n","\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 29515/29515 [00:00<00:00, 145663.83it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n","\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4422102/4422102 [00:01<00:00, 2663348.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n","\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 5148/5148 [00:00<00:00, 7755846.62it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n","\n","Epoch [1/10], Step [100/600], Loss: 0.5247\n","Epoch [1/10], Step [200/600], Loss: 0.5378\n","Epoch [1/10], Step [300/600], Loss: 0.3769\n","Epoch [1/10], Step [400/600], Loss: 0.5244\n","Epoch [1/10], Step [500/600], Loss: 0.3824\n","Epoch [1/10], Step [600/600], Loss: 0.4986\n","Epoch [2/10], Step [100/600], Loss: 0.3900\n","Epoch [2/10], Step [200/600], Loss: 0.2351\n","Epoch [2/10], Step [300/600], Loss: 0.4112\n","Epoch [2/10], Step [400/600], Loss: 0.2694\n","Epoch [2/10], Step [500/600], Loss: 0.2634\n","Epoch [2/10], Step [600/600], Loss: 0.3660\n","Epoch [3/10], Step [100/600], Loss: 0.2966\n","Epoch [3/10], Step [200/600], Loss: 0.2478\n","Epoch [3/10], Step [300/600], Loss: 0.3092\n","Epoch [3/10], Step [400/600], Loss: 0.2618\n","Epoch [3/10], Step [500/600], Loss: 0.1927\n","Epoch [3/10], Step [600/600], Loss: 0.3007\n","Epoch [4/10], Step [100/600], Loss: 0.1889\n","Epoch [4/10], Step [200/600], Loss: 0.2163\n","Epoch [4/10], Step [300/600], Loss: 0.1171\n","Epoch [4/10], Step [400/600], Loss: 0.2191\n","Epoch [4/10], Step [500/600], Loss: 0.3026\n","Epoch [4/10], Step [600/600], Loss: 0.2666\n","Epoch [5/10], Step [100/600], Loss: 0.1456\n","Epoch [5/10], Step [200/600], Loss: 0.2877\n","Epoch [5/10], Step [300/600], Loss: 0.2439\n","Epoch [5/10], Step [400/600], Loss: 0.2424\n","Epoch [5/10], Step [500/600], Loss: 0.2503\n","Epoch [5/10], Step [600/600], Loss: 0.2297\n","Epoch [6/10], Step [100/600], Loss: 0.1684\n","Epoch [6/10], Step [200/600], Loss: 0.3256\n","Epoch [6/10], Step [300/600], Loss: 0.3273\n","Epoch [6/10], Step [400/600], Loss: 0.2540\n","Epoch [6/10], Step [500/600], Loss: 0.2499\n","Epoch [6/10], Step [600/600], Loss: 0.2304\n","Epoch [7/10], Step [100/600], Loss: 0.2637\n","Epoch [7/10], Step [200/600], Loss: 0.2497\n","Epoch [7/10], Step [300/600], Loss: 0.1156\n","Epoch [7/10], Step [400/600], Loss: 0.2090\n","Epoch [7/10], Step [500/600], Loss: 0.1841\n","Epoch [7/10], Step [600/600], Loss: 0.1438\n","Epoch [8/10], Step [100/600], Loss: 0.2991\n","Epoch [8/10], Step [200/600], Loss: 0.2433\n","Epoch [8/10], Step [300/600], Loss: 0.2694\n","Epoch [8/10], Step [400/600], Loss: 0.1896\n","Epoch [8/10], Step [500/600], Loss: 0.1507\n","Epoch [8/10], Step [600/600], Loss: 0.2108\n","Epoch [9/10], Step [100/600], Loss: 0.2056\n","Epoch [9/10], Step [200/600], Loss: 0.1876\n","Epoch [9/10], Step [300/600], Loss: 0.1745\n","Epoch [9/10], Step [400/600], Loss: 0.1740\n","Epoch [9/10], Step [500/600], Loss: 0.1901\n","Epoch [9/10], Step [600/600], Loss: 0.1922\n","Epoch [10/10], Step [100/600], Loss: 0.2878\n","Epoch [10/10], Step [200/600], Loss: 0.1932\n","Epoch [10/10], Step [300/600], Loss: 0.2064\n","Epoch [10/10], Step [400/600], Loss: 0.0998\n","Epoch [10/10], Step [500/600], Loss: 0.1229\n","Epoch [10/10], Step [600/600], Loss: 0.2867\n","Test Accuracy of the model on the 10000 test images: 92.86 %\n"]}],"source":["import torch\n","from torch import nn\n","from torch.nn import functional as f\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Hyper-parameters\n","num_epochs = 10\n","batch_size = 100\n","learning_rate = 0.001\n","\n","# FashionMNIST dataset\n","train_dataset = torchvision.datasets.FashionMNIST(root='./data',\n","                                           train=True,\n","                                           transform=transforms.ToTensor(),\n","                                           download=True)\n","\n","test_dataset = torchvision.datasets.FashionMNIST(root='./data',\n","                                          train=False,\n","                                          transform=transforms.ToTensor())\n","\n","# Data loader\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n","                                           batch_size=batch_size,\n","                                           shuffle=True)\n","\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n","                                          batch_size=batch_size,\n","                                          shuffle=False)\n","\n","class net(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv2d_32 = nn.Conv2d(1,32,3,padding=1)\n","        self.conv2d_64 = nn.Conv2d(32,64,3,padding=1)\n","        self.max2d     = nn.MaxPool2d(2,2)\n","        self.conv2d_128 = nn.Conv2d(64,128,3,padding=1)\n","        self.conv2d_256 = nn.Conv2d(128,256,3, stride = 2,padding=1)\n","        self.linear1    = nn.Linear(3*3*256, 256)\n","        self.linear2    = nn.Linear(256,64)\n","        self.linear3    = nn.Linear(64,10)\n","        self.batch2d1     = nn.BatchNorm2d(64)\n","        self.batch2d2    = nn.BatchNorm2d(256)\n","        self.batch1d     = nn.BatchNorm1d(64)\n","        self.drop      = nn.Dropout(p=0.3)\n","        self.flat      = nn.Flatten()\n","\n","    def forward(self,x):\n","            x = x.view(-1,1,28,28)\n","            x = f.relu(self.conv2d_32(x))\n","            x = f.relu(self.conv2d_64(x))\n","            x = self.batch2d1(x)\n","            x = f.relu(self.max2d(x))\n","            x = self.drop(x)\n","\n","            x = f.relu(self.conv2d_128(x))\n","            x = f.relu(self.conv2d_256(x))\n","            x = self.batch2d2(x)\n","            x = f.relu(self.max2d(x))\n","            x = self.drop(x)\n","\n","            x = self.flat(x)\n","            x = f.relu(self.linear1(x))\n","            x = self.drop(x)\n","            x = f.relu(self.linear2(x))\n","            x = self.drop(x)\n","            x = self.batch1d(x)\n","            x = f.log_softmax(self.linear3(x), dim=1)\n","            return(x)\n","\n","model = net().to(device)\n","\n","# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Train the model\n","total_step = len(train_loader)\n","for epoch in range(num_epochs):\n","    for i, (images, labels) in enumerate(train_loader):\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        # Forward pass\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","\n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if (i+1) % 100 == 0:\n","            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n","                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n","\n","# Test the model\n","model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n","with torch.no_grad():\n","    correct = 0\n","    total = 0\n","    for images, labels in test_loader:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))"]},{"cell_type":"markdown","source":["Batch Normalization 클래스\n","- 학습 도중 하위 계층의 가중치가 변함에 따라 상위 계층이 받는 입력 데이터의 분포가 바뀌는 현상을 방지하기 위하여 각 계층의 입력을 정규화\n","- 배치 정규화는 딥 러닝 모델에서 매우 중요한 역할을 하는 요소로, 각 레이어의 입력을 정규화하여 학습을 더 안정적이고 빠르게 만드는 기술. - 이는 그래디언트 소실 문제를 완화하고, 일반적으로 더 빠른 훈련 속도와 더 나은 최종 성능을 가져오며, 규제 효과도 가진다.\n","- nn.BatchNorm2d(64)는  2차원 입력에 대한 배치 정규화를 수행하며  64개의 채널에 대해 배치 정규화를 적용\n","- 이 계층을 통과하면 각 채널의 데이터가 평균 0과 분산 1을 갖도록 정규화"],"metadata":{"id":"BhoOH6Cof6pC"}},{"cell_type":"code","source":["from torch import nn # 신경망 모듈을 포함하는 서브패키지\n","from torch.nn import functional as f\n","import torchvision # 컴퓨터 비전용 데이터셋 모델 등을 포함하는 라이브러리\n","import torchvision.transforms as transforms # 데이터 전처리를 위한 변환 함수들을 포함하는 서브 패키지\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","num_epochs = 10\n","batch_size = 100\n","learning_rate = 0.001\n","\n","train_dataset = torchvision.datasets.FashionMNIST( root = './data',\n","                                                  train = True,\n","                                                   transform = transforms.ToTensor(),\n","                                                   download = True)\n","\n","test_dataset = torchvision.datasets.FashionMNIST( root = './data',\n","                                                  train = False,\n","                                                   transform = transforms.ToTensor())\n","\n","train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n","                                           batch_size = batch_size,\n","                                           shuffle = True)\n","\n","test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n","                                           batch_size = batch_size,\n","                                           shuffle = False)\n","\n","# 신경망 모델정의\n","\n","class net(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv2d_32 = nn.Conv2d(1,32,3,padding=1)\n","        self.conv2d_64 = nn.Conv2d(32,64,3,padding=1)\n","        self.max2d     = nn.MaxPool2d(2,2)\n","        self.conv2d_128 = nn.Conv2d(64,128,3,padding=1)\n","        self.conv2d_256 = nn.Conv2d(128,256,3, stride = 2,padding=1)\n","        self.linear1    = nn.Linear(3*3*256, 256)\n","        self.linear2    = nn.Linear(256,64)\n","        self.linear3    = nn.Linear(64,10)\n","        self.batch2d1     = nn.BatchNorm2d(64)\n","        self.batch2d2    = nn.BatchNorm2d(256)\n","        self.batch1d     = nn.BatchNorm1d(64)\n","        self.drop      = nn.Dropout(p=0.3)\n","        self.flat      = nn.Flatten()\n","\n","    #  데이터가 모델을 통과할때의 순전파 연산을 정의\n","\n","    def forward(self,x):\n","            x = x.view(-1,1,28,28)\n","            x = f.relu(self.conv2d_32(x))\n","            x = f.relu(self.conv2d_64(x))\n","            x = self.batch2d1(x)\n","            x = f.relu(self.max2d(x))\n","            x = self.drop(x)\n","\n","            x = f.relu(self.conv2d_128(x))\n","            x = f.relu(self.conv2d_256(x))\n","            x = self.batch2d2(x)\n","            x = f.relu(self.max2d(x))\n","            x = self.drop(x)\n","\n","            x = self.flat(x)\n","            x = f.relu(self.linear1(x))\n","            x = self.drop(x)\n","            x = f.relu(self.linear2(x))\n","            x = self.drop(x)\n","            x = self.batch1d(x)\n","            x = f.log_softmax(self.linear3(x), dim=1)\n","            return(x)\n","\n","model = net().to(device) # 모델 인스턴스화 및 디바이스로 이동\n","\n","# Loss and optimizer\n","criterion = nn.CrossEntropyLoss() # 분류작업에 대한 손실함수\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) # 모델의 파라미터를 최적화\n","\n","# Train the model\n","# 주어진 에폭 수에 대해 데이터 로더를 반복하면서 수전파 손실 계산 역전파 옵티마이저 업데이트를 수행\n","total_step = len(train_loader)\n","for epoch in range(num_epochs):\n","    for i, (images, labels) in enumerate(train_loader):\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        # Forward pass\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","\n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if (i+1) % 100 == 0:\n","            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n","                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n","\n","# Test the model\n","# 테스트 데이터 로드를 사용하여 모델의 출력을 얻고 정확도를 계산\n","model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n","with torch.no_grad():\n","    correct = 0\n","    total = 0\n","    for images, labels in test_loader:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs.data, 1) # 각 샘플에 대해 가장 확률이 높은 클래스와 인덱스를 predicted에 저장 1은 확률을 나타내는 차원\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TKyT6kj_grQi","executionInfo":{"status":"ok","timestamp":1689727668483,"user_tz":-540,"elapsed":99075,"user":{"displayName":"므름표","userId":"11964526023051922071"}},"outputId":"fdea39fa-f6b9-4b46-90fa-cfb89354524f"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/10], Step [100/600], Loss: 0.8331\n","Epoch [1/10], Step [200/600], Loss: 0.5083\n","Epoch [1/10], Step [300/600], Loss: 0.6214\n","Epoch [1/10], Step [400/600], Loss: 0.5235\n","Epoch [1/10], Step [500/600], Loss: 0.2782\n","Epoch [1/10], Step [600/600], Loss: 0.3757\n","Epoch [2/10], Step [100/600], Loss: 0.3708\n","Epoch [2/10], Step [200/600], Loss: 0.3382\n","Epoch [2/10], Step [300/600], Loss: 0.3006\n","Epoch [2/10], Step [400/600], Loss: 0.2493\n","Epoch [2/10], Step [500/600], Loss: 0.3237\n","Epoch [2/10], Step [600/600], Loss: 0.2006\n","Epoch [3/10], Step [100/600], Loss: 0.3723\n","Epoch [3/10], Step [200/600], Loss: 0.2400\n","Epoch [3/10], Step [300/600], Loss: 0.2604\n","Epoch [3/10], Step [400/600], Loss: 0.2685\n","Epoch [3/10], Step [500/600], Loss: 0.1364\n","Epoch [3/10], Step [600/600], Loss: 0.2590\n","Epoch [4/10], Step [100/600], Loss: 0.1580\n","Epoch [4/10], Step [200/600], Loss: 0.3621\n","Epoch [4/10], Step [300/600], Loss: 0.1649\n","Epoch [4/10], Step [400/600], Loss: 0.3024\n","Epoch [4/10], Step [500/600], Loss: 0.2094\n","Epoch [4/10], Step [600/600], Loss: 0.4245\n","Epoch [5/10], Step [100/600], Loss: 0.1105\n","Epoch [5/10], Step [200/600], Loss: 0.1837\n","Epoch [5/10], Step [300/600], Loss: 0.2215\n","Epoch [5/10], Step [400/600], Loss: 0.3558\n","Epoch [5/10], Step [500/600], Loss: 0.2498\n","Epoch [5/10], Step [600/600], Loss: 0.2365\n","Epoch [6/10], Step [100/600], Loss: 0.1334\n","Epoch [6/10], Step [200/600], Loss: 0.1037\n","Epoch [6/10], Step [300/600], Loss: 0.4184\n","Epoch [6/10], Step [400/600], Loss: 0.2071\n","Epoch [6/10], Step [500/600], Loss: 0.3465\n","Epoch [6/10], Step [600/600], Loss: 0.1939\n","Epoch [7/10], Step [100/600], Loss: 0.2863\n","Epoch [7/10], Step [200/600], Loss: 0.3662\n","Epoch [7/10], Step [300/600], Loss: 0.2625\n","Epoch [7/10], Step [400/600], Loss: 0.0913\n","Epoch [7/10], Step [500/600], Loss: 0.1798\n","Epoch [7/10], Step [600/600], Loss: 0.1549\n","Epoch [8/10], Step [100/600], Loss: 0.1727\n","Epoch [8/10], Step [200/600], Loss: 0.2302\n","Epoch [8/10], Step [300/600], Loss: 0.2064\n","Epoch [8/10], Step [400/600], Loss: 0.1803\n","Epoch [8/10], Step [500/600], Loss: 0.1677\n","Epoch [8/10], Step [600/600], Loss: 0.2626\n","Epoch [9/10], Step [100/600], Loss: 0.1611\n","Epoch [9/10], Step [200/600], Loss: 0.2574\n","Epoch [9/10], Step [300/600], Loss: 0.2148\n","Epoch [9/10], Step [400/600], Loss: 0.1894\n","Epoch [9/10], Step [500/600], Loss: 0.2308\n","Epoch [9/10], Step [600/600], Loss: 0.1506\n","Epoch [10/10], Step [100/600], Loss: 0.1778\n","Epoch [10/10], Step [200/600], Loss: 0.1085\n","Epoch [10/10], Step [300/600], Loss: 0.2609\n","Epoch [10/10], Step [400/600], Loss: 0.2348\n","Epoch [10/10], Step [500/600], Loss: 0.1278\n","Epoch [10/10], Step [600/600], Loss: 0.1503\n","Test Accuracy of the model on the 10000 test images: 92.43 %\n"]}]},{"cell_type":"markdown","source":["keras 적용"],"metadata":{"id":"QAKkxmIphWgM"}},{"cell_type":"code","source":["import tensorflow as tf\n","\n","fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()\n","(train_images, train_labels), (test_images, test_labels) = fashion_mnist"],"metadata":{"id":"uhw-W7xlwiyp","executionInfo":{"status":"ok","timestamp":1689730421537,"user_tz":-540,"elapsed":495,"user":{"displayName":"므름표","userId":"11964526023051922071"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["print(train_images[0])\n","print(test_images[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zFa9GqFMw72_","executionInfo":{"status":"ok","timestamp":1689730433190,"user_tz":-540,"elapsed":1073,"user":{"displayName":"므름표","userId":"11964526023051922071"}},"outputId":"57eee2a7-7730-4634-ae48-bfc1c60c9ad1"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0   1   0   0  13  73   0\n","    0   1   4   0   0   0   0   1   1   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0   3   0  36 136 127  62\n","   54   0   0   0   1   3   4   0   0   3]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0   6   0 102 204 176 134\n","  144 123  23   0   0   0   0  12  10   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 155 236 207 178\n","  107 156 161 109  64  23  77 130  72  15]\n"," [  0   0   0   0   0   0   0   0   0   0   0   1   0  69 207 223 218 216\n","  216 163 127 121 122 146 141  88 172  66]\n"," [  0   0   0   0   0   0   0   0   0   1   1   1   0 200 232 232 233 229\n","  223 223 215 213 164 127 123 196 229   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0   0 183 225 216 223 228\n","  235 227 224 222 224 221 223 245 173   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0   0 193 228 218 213 198\n","  180 212 210 211 213 223 220 243 202   0]\n"," [  0   0   0   0   0   0   0   0   0   1   3   0  12 219 220 212 218 192\n","  169 227 208 218 224 212 226 197 209  52]\n"," [  0   0   0   0   0   0   0   0   0   0   6   0  99 244 222 220 218 203\n","  198 221 215 213 222 220 245 119 167  56]\n"," [  0   0   0   0   0   0   0   0   0   4   0   0  55 236 228 230 228 240\n","  232 213 218 223 234 217 217 209  92   0]\n"," [  0   0   1   4   6   7   2   0   0   0   0   0 237 226 217 223 222 219\n","  222 221 216 223 229 215 218 255  77   0]\n"," [  0   3   0   0   0   0   0   0   0  62 145 204 228 207 213 221 218 208\n","  211 218 224 223 219 215 224 244 159   0]\n"," [  0   0   0   0  18  44  82 107 189 228 220 222 217 226 200 205 211 230\n","  224 234 176 188 250 248 233 238 215   0]\n"," [  0  57 187 208 224 221 224 208 204 214 208 209 200 159 245 193 206 223\n","  255 255 221 234 221 211 220 232 246   0]\n"," [  3 202 228 224 221 211 211 214 205 205 205 220 240  80 150 255 229 221\n","  188 154 191 210 204 209 222 228 225   0]\n"," [ 98 233 198 210 222 229 229 234 249 220 194 215 217 241  65  73 106 117\n","  168 219 221 215 217 223 223 224 229  29]\n"," [ 75 204 212 204 193 205 211 225 216 185 197 206 198 213 240 195 227 245\n","  239 223 218 212 209 222 220 221 230  67]\n"," [ 48 203 183 194 213 197 185 190 194 192 202 214 219 221 220 236 225 216\n","  199 206 186 181 177 172 181 205 206 115]\n"," [  0 122 219 193 179 171 183 196 204 210 213 207 211 210 200 196 194 191\n","  195 191 198 192 176 156 167 177 210  92]\n"," [  0   0  74 189 212 191 175 172 175 181 185 188 189 188 193 198 204 209\n","  210 210 211 188 188 194 192 216 170   0]\n"," [  2   0   0   0  66 200 222 237 239 242 246 243 244 221 220 193 191 179\n","  182 182 181 176 166 168  99  58   0   0]\n"," [  0   0   0   0   0   0   0  40  61  44  72  41  35   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0]]\n","[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","    0   3   1   0   0   7   0  37   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0   0   1   2   0  27  84\n","   11   0   0   0   0   0   0 119   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0  88 143\n","  110   0   0   0   0  22  93 106   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0   0   4   0  53 129 120\n","  147 175 157 166 135 154 168 140   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0   2   0  11 137 130 128\n","  160 176 159 167 178 149 151 144   0   0]\n"," [  0   0   0   0   0   0   1   0   2   1   0   3   0   0 115 114 106 137\n","  168 153 156 165 167 143 157 158  11   0]\n"," [  0   0   0   0   1   0   0   0   0   0   3   0   0  89 139  90  94 153\n","  149 131 151 169 172 143 159 169  48   0]\n"," [  0   0   0   0   0   0   2   4   1   0   0   0  98 136 110 109 110 162\n","  135 144 149 159 167 144 158 169 119   0]\n"," [  0   0   2   2   1   2   0   0   0   0  26 108 117  99 111 117 136 156\n","  134 154 154 156 160 141 147 156 178   0]\n"," [  3   0   0   0   0   0   0  21  53  92 117 111 103 115 129 134 143 154\n","  165 170 154 151 154 143 138 150 165  43]\n"," [  0   0  23  54  65  76  85 118 128 123 111 113 118 127 125 139 133 136\n","  160 140 155 161 144 155 172 161 189  62]\n"," [  0  68  94  90 111 114 111 114 115 127 135 136 143 126 127 151 154 143\n","  148 125 162 162 144 138 153 162 196  58]\n"," [ 70 169 129 104  98 100  94  97  98 102 108 106 119 120 129 149 156 167\n","  190 190 196 198 198 187 197 189 184  36]\n"," [ 16 126 171 188 188 184 171 153 135 120 126 127 146 185 195 209 208 255\n","  209 177 245 252 251 251 247 220 206  49]\n"," [  0   0   0  12  67 106 164 185 199 210 211 210 208 190 150  82   8   0\n","    0   0 178 208 188 175 162 158 151  11]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0]]\n"]}]},{"cell_type":"code","source":[" train_images, test_images = train_images / 255.0, test_images/ 255.0"],"metadata":{"id":"yOvzP7wHxQ1-","executionInfo":{"status":"ok","timestamp":1689730434760,"user_tz":-540,"elapsed":8,"user":{"displayName":"므름표","userId":"11964526023051922071"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["model = tf.keras.models.Sequential([\n","    tf.keras.layers.Flatten(input_shape = (28, 28)),\n","    tf.keras.layers.Dense(512, activation = 'relu'),\n","    tf.keras.layers.Dense(10, activation = 'softmax')\n","])"],"metadata":{"id":"NtmiKl-IyD-W","executionInfo":{"status":"ok","timestamp":1689730527917,"user_tz":-540,"elapsed":2843,"user":{"displayName":"므름표","userId":"11964526023051922071"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3KwF7OqWyaQ-","executionInfo":{"status":"ok","timestamp":1689730532604,"user_tz":-540,"elapsed":14,"user":{"displayName":"므름표","userId":"11964526023051922071"}},"outputId":"3aa8c605-ec8b-4cbb-b4c3-ba31749492bf"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," flatten (Flatten)           (None, 784)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               401920    \n","                                                                 \n"," dense_1 (Dense)             (None, 10)                5130      \n","                                                                 \n","=================================================================\n","Total params: 407,050\n","Trainable params: 407,050\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["model.compile(optimizer = 'adam',\n","              loss = 'sparse_categorical_crossentropy',\n","              metrics = ['accuracy'])"],"metadata":{"id":"LDeByINUyb6W","executionInfo":{"status":"ok","timestamp":1689730689106,"user_tz":-540,"elapsed":4,"user":{"displayName":"므름표","userId":"11964526023051922071"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["model.fit(train_images, train_labels, epochs = 5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N1xf-EDmyqcv","executionInfo":{"status":"ok","timestamp":1689730713856,"user_tz":-540,"elapsed":23561,"user":{"displayName":"므름표","userId":"11964526023051922071"}},"outputId":"ec53aae3-18a3-4290-e55f-1859c198a773"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","1875/1875 [==============================] - 6s 2ms/step - loss: 0.4760 - accuracy: 0.8281\n","Epoch 2/5\n","1875/1875 [==============================] - 4s 2ms/step - loss: 0.3608 - accuracy: 0.8686\n","Epoch 3/5\n","1875/1875 [==============================] - 4s 2ms/step - loss: 0.3200 - accuracy: 0.8814\n","Epoch 4/5\n","1875/1875 [==============================] - 4s 2ms/step - loss: 0.2985 - accuracy: 0.8890\n","Epoch 5/5\n","1875/1875 [==============================] - 4s 2ms/step - loss: 0.2799 - accuracy: 0.8968\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7aa3843d3430>"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["loss, accuracy = model.evaluate(test_images, test_labels)\n","print(loss, accuracy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pA29-J8dyxUm","executionInfo":{"status":"ok","timestamp":1689730714529,"user_tz":-540,"elapsed":696,"user":{"displayName":"므름표","userId":"11964526023051922071"}},"outputId":"18416481-a568-4a09-e676-803e127e15e4"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["313/313 [==============================] - 1s 2ms/step - loss: 0.3425 - accuracy: 0.8783\n","0.3425152003765106 0.8783000111579895\n"]}]},{"cell_type":"markdown","source":["https://github.com/ratsgo/ratsnlp/tree/master/ratsnlp/nlpbook"],"metadata":{"id":"Hojd7K4Hy_lt"}},{"cell_type":"markdown","source":["## 트랜스퍼 러닝\n","- 트랜스퍼 러닝(transfer learning)이란 특정 태스크를 학습한 모델을 다른 태스크 수행에 재사용하는 기법\n","- 트랜스퍼 러닝이 주목받게 된 것은 업스트림 태스크와 프리트레인 덕분. 자연어의 풍부한 문맥(context)을 모델에 내재화하고 이 모델을 다양한 다운스트림 태스크에 적용해 그 성능을 대폭 끌어올림\n","- 대표적인 업스트림 태스크 가운데 하나가 다음 단어 맞추기입니다. GPT 계열 모델이 바로 이 태스크로 프리트레인을 수행\n","- 다른 업스트림 태스크로는 빈칸 채우기가 있습니다. BERT 계열 모델이 바로 이 태스크로 프리트레인을 수행. ‘빈칸 채우기’로 업스트림 태스크를 수행한 모델을 마스크 언어 모델(Masked Language Model)\n","- 다음 단어 맞히기, 빈칸 채우기 같은 업스트림 태스크는 강력한 힘을 지니며 뉴스, 웹 문서, 백과사전 등 글만 있으면 수작업 없이도 다량의 학습 데이터를 아주 싼값에 만들어 낼 수 있다. 이처럼 데이터 내에서 정답을 만들고 이를 바탕으로 모델을 학습하는 방법을 자기 지도 학습(self-supervised learning)이라고 함.\n","- 다운스트림 태스크의 학습 방식은 모두 파인튜닝(fine-tuning)이며 프리트레인을 마친 모델을 다운스트림 태스크에 맞게 업데이트하는 기법"],"metadata":{"id":"FBQeD2uL0_Pv"}},{"cell_type":"markdown","source":["## 바이트 페어 인코딩(Byte Pair Encoding, BPE)\n","- 원래 정보를 압축하는 알고리즘으로 제안되었는데 최근에는 자연어 처리 모델에 널리 쓰이고 있는 토큰화 기법\n","- 데이터에서 가장 많이 등장한 문자열을 병합해서 데이터를 압축하는 기법\n","\n","## 워드피스(wordpiece)\n","- 말뭉치에서 자주 등장한 문자열을 토큰으로 인식한다는 점에서 BPE와 본질적으로 유사. 다만 어휘 집합을 구축할 때 문자열을 병합하는 기준이 다다. 워드피스는 BPE처럼 단순히 빈도를 기준으로 병합하는 것이 아니라, 병합했을 때 말뭉치의 우도(likelihood)를 가장 높이는 쌍을 병합"],"metadata":{"id":"9j0gdKBJCH6i"}},{"cell_type":"code","source":[""],"metadata":{"id":"cBUJBlcdCMiK"},"execution_count":null,"outputs":[]}]}