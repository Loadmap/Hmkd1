{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8001accf",
   "metadata": {},
   "source": [
    "### 사이킷런을 이용하여 iris 데이터 품종 예측하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f0c47f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rc('font', family = 'Malgun Gothic')\n",
    "mpl.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0faad0f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "iris.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9927da76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]] \n",
      "\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2] \n",
      "\n",
      "None \n",
      "\n",
      "['setosa' 'versicolor' 'virginica'] \n",
      "\n",
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ... \n",
      "\n",
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'] \n",
      "\n",
      "iris.csv \n",
      "\n",
      "sklearn.datasets.data \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(iris.data, '\\n') # 각 열은 꽃잎의 길이, 꽃잎의 폭, 꽃받침의 길이, 꽃받침의 폭과 같은 특징을 나타냅니다.\n",
    "print(iris.target, '\\n') # 각 샘플의 클래스 레이블을 담고 있으며, 숫자로 표현\n",
    "print(iris.frame, '\\n') # 일부 버전의 pandas 데이터프레임으로 데이터셋을 표현할 때 사용될 수 있는 데이터프레임\n",
    "print(iris.target_names, '\\n') # 타겟 클래스(붓꽃의 종류)의 이름을 나타내는 문자열 배열\n",
    "print(iris.DESCR, '\\n') # Iris 데이터셋의 속성, 클래스 레이블 및 데이터셋의 기타 정보를 설명\n",
    "print(iris.feature_names, '\\n') # 특징 벡터의 각 열에 대한 설명을 나타내는 문자열 배열\n",
    "print(iris.filename, '\\n') # 데이터셋 파일의 경로와 파일 이름\n",
    "print(iris.data_module, '\\n') # 데이터셋을 로드하는 데 사용되는 모듈의 이름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4207c9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris target값:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "iris target명:  ['setosa' 'versicolor' 'virginica']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "2                4.7               3.2                1.3               0.2   \n",
       "3                4.6               3.1                1.5               0.2   \n",
       "4                5.0               3.6                1.4               0.2   \n",
       "\n",
       "   label  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# iris 데이터 세트 로딩\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "# iris .data는 iris 데이터 세트에서 feature 만으로 된 데이터를 numpy로 가지고 있습니다\n",
    "\n",
    "iris_data = iris.data\n",
    "\n",
    "# iris.target 은 iris 데이터 세트에서 결정값 데이터를 numpy로 가지고 있습니다\n",
    "\n",
    "iris_label = iris.target\n",
    "print('iris target값: ', iris_label)\n",
    "print('iris target명: ', iris.target_names)\n",
    "\n",
    "# irist 데이터 세트를 자세히 보기 위해 데이터 프레임으로 변환\n",
    "\n",
    "iris_df = pd.DataFrame(data = iris_data, columns = iris.feature_names)\n",
    "iris_df['label'] = iris.target\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d080f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   sepal length (cm)  150 non-null    float64\n",
      " 1   sepal width (cm)   150 non-null    float64\n",
      " 2   petal length (cm)  150 non-null    float64\n",
      " 3   petal width (cm)   150 non-null    float64\n",
      " 4   label              150 non-null    int32  \n",
      "dtypes: float64(4), int32(1)\n",
      "memory usage: 5.4 KB\n"
     ]
    }
   ],
   "source": [
    "iris_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cccaf032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train은 학습용 특징 데이터, y_train은 학습용 타겟 데이터, X_test는 테스트용 특징 데이터, y_test는 테스트용 타겟 데이터를 나타냅니다\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_data, iris_label, test_size = 0.2, random_state = 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d59dda5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e4705798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(random_state=11)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decisiontreeclassifier 객체 생성\n",
    "dt_clf = DecisionTreeClassifier(random_state = 11)\n",
    "\n",
    "# 학습수행\n",
    "dt_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d617ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습이 완료된 decisiontreeclassifier 객체에서 테스트 데이터 세트로 예측 수행\n",
    "\n",
    "pred = dt_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2886ea00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측정확도:  0.9333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print('예측정확도: {0: .4f}'.format(accuracy_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de92416",
   "metadata": {},
   "source": [
    "### 머신러닝\n",
    "- 데이터를 기반으로 학습하고 예측하는 알고리즘의 한 분야입니다. \n",
    "- 머신러닝은 컴퓨터가 명시적으로 프로그래밍되지 않고도 데이터에서 학습하고 예측할 수 있도록 합니다. \n",
    "- 머신러닝은 다양한 분야에서 사용되며, 학습의 핵심 사항은 다음과 같습니다.\n",
    "    - 데이터: 머신러닝은 데이터를 기반으로 학습합니다. 따라서 좋은 데이터가 좋은 모델의 핵심입니다.\n",
    "    - 알고리즘: 머신러닝에는 다양한 알고리즘이 있습니다. 각 알고리즘은 특정 유형의 문제에 더 적합합니다.\n",
    "    - 하이퍼파라미터: 알고리즘의 성능에 영향을 미치는 매개변수를 하이퍼파라미터라고 합니다. 하이퍼파라미터는 실험을 통해 최적화해야 합니다.\n",
    "    - 평가: 모델의 성능을 평가하는 것은 중요합니다. 모델의 성능을 평가하는 방법에는 여러 가지가 있습니다.\n",
    "    - 배포: 모델이 개발되면 배포해야 합니다. 모델을 배포하는 방법에는 여러 가지가 있습니다.\n",
    "\n",
    "### 기계 학습(ML) 학습 내용\n",
    "- 기계 학습의 기초: 여기에는 기계 학습의 기본 개념, 유형(지도 학습, 비지도 학습, 반지도 학습 및 강화 학습) 및 다양한 응용 프로그램에 대한 이해가 포함됩니다.\n",
    "- 지도 학습: 선형 회귀, 로지스틱 회귀, 결정 트리, 랜덤 포레스트, 그래디언트 부스팅, SVM(Support Vector Machines) 및 k-NN(k-nearest neighbours)과 같은 알고리즘에 대해 배웁니다. 이러한 알고리즘은 레이블이 지정된 학습 데이터를 사용하여 입력 기능을 기반으로 대상 변수를 예측하는 데 사용됩니다.\n",
    "- 비지도 학습: 여기에는 k-평균, 계층적 클러스터링 및 DBSCAN과 같은 클러스터링 기술에 대한 학습이 포함됩니다. 또한 PCA(Principal Component Analysis) 및 자동 인코더와 같은 차원 축소 기술에 대해서도 배웁니다.\n",
    "- 강화 학습: 에이전트가 환경과 상호 작용하여 결정을 내리는 방법을 배우는 강화 학습의 기본 사항을 배웁니다.\n",
    "- 평가 지표: 모델의 성능을 측정하는 방법을 이해하는 것이 중요합니다. 여기에는 정확도, 정밀도, 재현율, 분류 작업의 F1 점수, 평균 절대 오차, 회귀 작업의 평균 제곱근 오차와 같은 메트릭이 포함됩니다.\n",
    "- 신경망 및 딥러닝: 신경망, 역전파 및 CNN(Convolutional Neural Networks), RNN(Recurrent Neural Networks), Long Short-Term 메모리(LSTM) 네트워크 및 트랜스포머.\n",
    "- NLP(Natural Language Processing): 인간 언어 데이터를 처리하고 이해하는 기술을 포함합니다. 주제에는 텍스트 전처리, 단어 임베딩 및 시퀀스 간 모델이 포함됩니다.\n",
    "- 데이터 전처리: 누락된 데이터, 이상치 및 범주 데이터 처리를 포함하여 기계 학습을 위해 데이터를 전처리하는 기술을 다룹니다.\n",
    "- 피처 엔지니어링: 기존 변수에서 새로운 변수인 파생변수를 생성하고 모델에 가장 유용한 변수를 선택하는 방법을 배웁니다.\n",
    "- 편향-분산 트레이드오프: 과적합, 과소적합, 편향과 분산 사이의 균형에 대한 이해는 효과적인 모델을 구축하는 데 필수적입니다.\n",
    "- 정규화: 과적합을 방지하기 위한 L1 및 L2 정규화와 같은 기술을 다룹니다.\n",
    "- 모델 선택 및 하이퍼파라미터 조정: 교차 검증, GridSearch와 같은 기술을 포함하여 성능을 향상시키기 위해 올바른 모델을 선택하고 하이퍼파라미터를 조정하는 전략을 배우게 됩니다.\n",
    "- 기계 학습은 방대한 분야이며 기초부터 시작하여 점진적으로 지식을 확장하는 것이 좋습니다. 프로젝트를 통해 이러한 개념을 실제로 적용하는 것이 이해를 강화하는 좋은 방법입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cc002e",
   "metadata": {},
   "source": [
    "### 머신러닝 알고리즘\n",
    "- 크게 지도 학습, 비지도 학습, 강화 학습의 세 가지로 분류할 수 있습니다.\n",
    "- 지도 학습은 레이블이 있는 데이터를 사용하여 모델을 학습하는 방법입니다. 지도 학습 알고리즘에는 선형 회귀, 로지스틱 회귀, 의사 결정 트리, 랜덤 포레스트, 서포트 벡터 머신 등이 있습니다.\n",
    "- 비지도 학습은 레이블이 없는 데이터를 사용하여 모델을 학습하는 방법입니다. 비지도 학습 알고리즘에는 클러스터링, 차원 축소, 특징 추출 등이 있습니다.\n",
    "- 강화 학습은 에이전트가 환경과 상호 작용하여 보상을 최대화하는 방법을 학습하는 방법입니다. \n",
    "- 각 알고리즘의 로직과 기능\n",
    "    - 선형 회귀는 선형 방정식을 사용하여 데이터를 모델링하는 알고리즘입니다. 선형 회귀는 종속 변수와 독립 변수 간의 선형 관계를 예측하는 데 사용됩니다.\n",
    "    - 로지스틱 회귀는 로지스틱 함수를 사용하여 데이터를 모델링하는 알고리즘입니다. 로지스틱 회귀는 이진 분류 문제에 사용됩니다.\n",
    "    - 의사 결정 트리는 의사 결정 트리를 사용하여 데이터를 모델링하는 알고리즘입니다. 의사 결정 트리는 분류 및 회귀 문제에 사용됩니다.\n",
    "    - 랜덤 포레스트는 여러 의사 결정 트리를 결합하여 데이터를 모델링하는 알고리즘입니다. 랜덤 포레스트는 분류 및 회귀 문제에 사용됩니다.\n",
    "    - 서포트 벡터 머신은 서포트 벡터와 마진을 사용하여 데이터를 모델링하는 알고리즘입니다. 서포트 벡터 머신은 분류 및 회귀 문제에 사용됩니다.\n",
    "    - 클러스터링은 데이터를 유사한 그룹으로 그룹화하는 알고리즘입니다. 클러스터링은 데이터를 이해하고 데이터에서 패턴을 찾는 데 사용됩니다.\n",
    "    - 차원 축소는 고차원 데이터를 저차원 공간으로 투영하는 알고리즘입니다. 차원 축소는 데이터를 이해하고 데이터에서 패턴을 찾는 데 사용됩니다.\n",
    "    - 특징 추출은 데이터에서 새로운 특징을 추출하는 알고리즘입니다. 특징 추출은 데이터를 이해하고 데이터에서 패턴을 찾는 데 사용됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccca907",
   "metadata": {},
   "source": [
    "### 지도 학습\n",
    "- 기계 학습의 가장 일반적인 형태 중 하나이며 간단한 원리로 작동합니다.\n",
    "- 감독 학습의 원리\n",
    "    - 지도 학습에서 알고리즘은 레이블이 지정된 학습 데이터에서 학습하고 해당 데이터를 기반으로 예측을 수행합니다. \n",
    "    - 지도 학습 알고리즘은 알려진 입력 데이터 세트(특징)와 데이터에 대한 알려진 응답(대상 또는 레이블)을 취하고 모델을 훈련하여 새 데이터에 대한 응답에 대한 합리적인 예측을 생성합니다.\n",
    "- 지도 학습은 회귀 및 분류라는 두 가지 유형의 문제에 사용됩니다.\n",
    "    - 회귀: 회귀는 대상 또는 종속 변수가 연속적이거나 정렬된 전체 값일 때 사용됩니다. 회귀에 사용되는 알고리즘에는 선형 회귀, 결정 트리, 랜덤 포레스트 및 그라데이션 부스팅이 포함됩니다.\n",
    "    - 분류: 분류는 대상 변수가 범주형일 때, 간단히 말해서 입력 데이터를 범주로 분류할 때 사용합니다. 분류에 사용되는 알고리즘에는 Logistic Regression, Naive Bayes, Decision Trees, Random Forests, Support Vector Machines 및 Neural Networks가 포함됩니다.\n",
    "- 지도 학습 알고리즘이 작동하는 방식\n",
    "    - 데이터 수집: 데이터를 수집하고 전처리합니다. 데이터는 기능(입력)과 대상(출력)으로 나누어야 합니다. 이를 위해 사용되는 데이터는 출력 데이터 세트도 제공되기 때문에 레이블이 지정된 데이터라고 합니다.\n",
    "    - 모델 훈련: 알고리즘은 훈련 데이터를 통해 학습합니다. 기능과 대상 간의 패턴 또는 관계를 발견하려고 시도합니다.\n",
    "    - 모델 예측: 모델이 훈련되면 본 적이 없는 새로운 데이터의 결과를 예측하는 데 사용할 수 있습니다. 이 새 데이터에 대한 입력을 테스트 데이터라고 합니다.\n",
    "    - 평가: 모델의 예측을 실제 값과 비교하여 모델의 정확도를 평가합니다. 정확도, 정밀도, 재현율, F1 점수(분류용), 평균 절대 오차(MAE), 평균 제곱 오차(MSE), 평균 제곱근 오차(RMSE)(회귀용)와 같은 다양한 메트릭이 모델을 평가하는 데 사용됩니다.\n",
    "    - 조정: 모델의 성능이 만족스럽지 않으면 모델로 돌아가 매개변수를 조정하거나 다른 모델을 모두 선택해야 할 수 있습니다. 이 프로세스는 하이퍼파라미터 조정이라고도 합니다.\n",
    "    - 예측: 만족스러운 성능이 달성되면 이제 모델을 사용하여 보이지 않는 새로운 데이터를 예측할 수 있습니다.\n",
    "- 지도 학습의 핵심 측면은 레이블이 지정된 데이터를 사용하여 모델을 훈련한다는 것입니다. 즉, 입력(기능)과 올바른 출력(레이블)이 있고 모델은 입력을 출력에 매핑하는 기능을 학습합니다. 이 함수를 학습하면 새로운 입력을 출력에 매핑하는 데 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefb8e30",
   "metadata": {},
   "source": [
    "### 비지도학습\n",
    "- 비지도 학습(Unsupervised Learning)은 레이블이 지정되지 않은 데이터를 사용하여 모델을 학습하는 방법입니다. \n",
    "- 지도 학습과 달리 비지도 학습은 모델의 성능을 평가하기 위한 레이블이 필요하지 않습니다. \n",
    "- 비지도 학습 알고리즘\n",
    "    - 클러스터링: 클러스터링은 유사한 데이터 포인트를 그룹화하는 작업입니다. 클러스터링은 데이터를 이해하고 데이터에서 패턴을 찾는 데 사용됩니다.\n",
    "    - 차원 축소: 차원 축소는 고차원 데이터를 저차원 공간으로 투영하는 작업입니다. 차원 축소는 데이터를 이해하고 데이터에서 패턴을 찾는 데 사용됩니다.\n",
    "    - 특징 추출: 특징 추출은 데이터에서 새로운 특징을 추출하는 작업입니다. 특징 추출은 데이터를 이해하고 데이터에서 패턴을 찾는 데 사용됩니다.\n",
    "- 비지도 학습은 다음과 같은 다양한 알고리즘을 사용하여 수행할 수 있습니다.\n",
    "    - 클러스터링: 클러스터링 알고리즘에는 K-평균, 평균-점 클러스터링, DBSCAN 등이 있습니다.\n",
    "    - 차원 축소: 차원 축소 알고리즘에는 주성분 분석(PCA), 차원 축소, 비지도 특징 추출 등이 있습니다.\n",
    "    - 특징 추출: 특징 추출 알고리즘에는 주성분 분석(PCA), 선형 변환, 비선형 변환 등이 있습니다.\n",
    "- 비지도 학습은 머신 러닝에서 강력한 도구입니다. 비지도 학습은 데이터를 이해하고 데이터에서 패턴을 찾는 데 사용됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052230b7",
   "metadata": {},
   "source": [
    "### 사이킷런\n",
    "- 사이킷런(scikit-learn)은 파이썬 머신러닝 라이브러리 중 가장 많이 사용되는 라이브러리입니다. \n",
    "- 사이킷런은 파이썬 기반의 머신러닝을 위한 가장 쉽고 효율적인 개발 라이브러리를 제공합니다.\n",
    "\n",
    "- 사이킷런의 특징\n",
    "    - 파이썬 기반의 다른 머신러닝 패키지도 사이킷런 스타일의 API를 지향할 정도로 쉽고 가장 파이썬스러운 API를 제공합니다.\n",
    "    - 머신러닝을 위한 매우 다양한 알고리즘과 개발을 위한 편리한 프레임워크와 API를 제공합니다.\n",
    "    - 많은 머신러닝 알고리즘이 효율적으로 구현되어 있기에 머신러닝을 처음 배울 때 사용하기 매우 좋습니다.\n",
    "    - 사이킷런은 오픈 소스 라이브러리이기 때문에 누구나 자유롭게 사용할 수 있습니다.\n",
    "    - 사이킷런은 다음과 같은 다양한 머신러닝 알고리즘을 제공합니다.\n",
    "        - 분류\n",
    "        - 회귀\n",
    "        - 클러스터링\n",
    "        - 차원 축소\n",
    "        - 특징 추출\n",
    "        - 모델 선택 및 평가\n",
    "    - 사이킷런은 또한 다음과 같은 다양한 프레임워크와 API를 제공합니다.\n",
    "        - 데이터 전처리\n",
    "        - 모델 학습 및 예측\n",
    "        - 모델 저장 및 복원\n",
    "        - 모델 모니터링\n",
    "        - 모델 배포"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8e59ac",
   "metadata": {},
   "source": [
    "### Python의 기계 학습 라이브러리인 Scikit-Learn을 효과적으로 이해하기 위해 배워야 할 주요 사항\n",
    "\n",
    "- Python 프로그래밍: Scikit-Learn은 Python 라이브러리이므로 Python 숙련도가 중요합니다. 여기에는 데이터 유형, 제어 흐름, 기능 이해 및 패키지 작업이 포함됩니다.\n",
    "- 데이터 구조: 목록, 사전 및 집합과 같은 Python의 기본 데이터 구조를 이해하는 것이 중요합니다. 데이터 분석 라이브러리인 Pandas는 특히 DataFrames와 함께 Scikit-Learn에서 사용할 데이터 조작에 필수적입니다.\n",
    "- NumPy 및 Matplotlib: Scikit-Learn은 숫자 데이터를 효율적으로 처리하는 데 사용되는 NumPy 배열과 잘 작동합니다. 플로팅 라이브러리인 Matplotlib는 데이터를 시각화하고 결과를 모델링하는 데 사용됩니다.\n",
    "- 기본 통계 및 확률: 이들은 기계 학습의 빌딩 블록입니다. 주요 개념에는 평균, 중앙값, 모드, 표준 편차, 확률 분포, 가설 테스트 및 상관 계수가 포함됩니다.\n",
    "- 머신 러닝 개념: 지도 학습, 비지도 학습, 강화 학습 등 머신 러닝의 기본 사항을 숙지합니다. 회귀, 분류, 클러스터링, 차원 감소 및 앙상블 방법과 같은 일반적인 알고리즘 및 개념을 이해합니다.\n",
    "- Scikit-Learn API: Scikit-Learn API와 일관된 인터페이스를 이해하는 것이 중요합니다. 여기에는 라이브러리를 가져오고, 모델을 만들고, 모델을 데이터에 맞추고, 새 데이터 포인트를 예측하고, 모델의 성능을 평가하는 방법을 아는 것이 포함됩니다.\n",
    "- 데이터 전처리: 크기 조정, 정규화, 범주형 변수 인코딩 및 누락된 값 처리를 위한 Scikit-Learn의 전처리 도구를 사용하는 방법을 배웁니다.\n",
    "- 모델 평가 및 조정: 교차 검증, 혼동 행렬, ROC 곡선, 정밀도, 재현율, F1 점수 등과 같은 모델 평가를 위한 Scikit-Learn 도구를 사용하는 방법을 이해합니다. GridSearch 및 RandomizedSearch와 같은 기술을 사용하여 모델을 조정하는 방법을 알아보세요.\n",
    "- 파이프라인 생성: Scikit-Learn은 전처리 및 모델 교육 프로세스를 간소화하는 파이프라인을 생성하는 유틸리티를 제공합니다. 이것은 효율적이고 재현 가능한 코드를 만들기 위해 배워야 할 중요한 측면입니다.\n",
    "- 딥 러닝: Scikit-Learn은 주로 딥 러닝에 사용되지 않지만 알아두면 유용할 수 있는 신경망에 대한 일부 지원을 제공합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b8d404",
   "metadata": {},
   "source": [
    "### 사이킷런의 주요 모듈\n",
    "- model_selection: 학습 데이터와 테스트 데이터를 분리하거나 교차 검증, 그리고 Estimator의 하이퍼 파라미터를 튜닝하기 위해서 다양한 함수와 클래스를 제공합니다.\n",
    "- preprocessing: 데이터를 정규화, 스케일링, 인코딩하는 등 다양한 기능을 제공합니다.\n",
    "- datasets: 다양한 데이터 세트를 제공합니다.\n",
    "- linear_model: 선형 회귀, 로지스틱 회귀, 릿지 회귀, 라쏘 회귀 등 다양한 선형 모델을 제공합니다.\n",
    "- tree: 의사 결정 트리, 랜덤 포레스트, 그래디언트 부스팅 트리 등 다양한 트리 기반 모델을 제공합니다.\n",
    "- svm: 서포트 벡터 머신, 커널 서포트 벡터 머신 등 다양한 서포트 벡터 머신을 제공합니다.\n",
    "- neighbors: K-최근접 이웃 알고리즘을 제공합니다.\n",
    "- ensemble: 앙상블 방법을 제공합니다.\n",
    "- cluster: 클러스터링 알고리즘을 제공합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1fc7c75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.utils.Bunch'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris_data = load_iris()\n",
    "\n",
    "print(type(iris_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "241534a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "4\n",
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'] \n",
      "\n",
      "<class 'numpy.ndarray'>\n",
      "3\n",
      "['setosa' 'versicolor' 'virginica'] \n",
      "\n",
      "<class 'numpy.ndarray'>\n",
      "(150, 4)\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]] \n",
      "\n",
      "<class 'numpy.ndarray'>\n",
      "(150,)\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(type(iris_data.feature_names))\n",
    "print(len(iris_data.feature_names))\n",
    "print(iris_data.feature_names, '\\n')\n",
    "\n",
    "print(type(iris_data.target_names))\n",
    "print(len(iris_data.target_names))\n",
    "print(iris_data.target_names, '\\n')\n",
    "\n",
    "print(type(iris_data.data))\n",
    "print(iris_data.data.shape)\n",
    "print(iris_data['data'], '\\n')\n",
    "\n",
    "print(type(iris_data.target))\n",
    "print(iris_data.target.shape)\n",
    "print(iris_data.target, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6529be",
   "metadata": {},
   "source": [
    "### 사이킷런의 model_selection 모듈\n",
    "model_selection 모듈은 머신러닝 모델을 개발하고 평가하는 데 유용한 도구입니다. model_selection 모듈을 사용하면 데이터를 더 잘 이해하고 더 나은 모델을 개발할 수 있습니다.\n",
    "- 학습 데이터와 테스트 데이터를 분리하거나 교차 검증, 그리고 Estimator의 하이퍼 파라미터를 튜닝하기 위해서 다양한 함수와 클래스를 제공합니다.\n",
    "\n",
    "- 학습 데이터와 테스트 데이터를 분리하는 방법에는 여러 가지가 있습니다. 가장 일반적인 방법은 train_test_split() 함수를 사용하는 것입니다. train_test_split() 함수는 학습 데이터와 테스트 데이터를 70:30의 비율로 분리합니다.\n",
    "\n",
    "- 교차 검증은 모델의 성능을 평가하는 방법입니다. 교차 검증은 데이터를 여러 개의 폴드로 분리한 다음 각 폴드를 테스트 데이터로 사용하고 나머지 폴드를 학습 데이터로 사용하여 모델을 학습하는 방법입니다. 교차 검증은 모델의 성능을 더 정확하게 평가할 수 있습니다.\n",
    "\n",
    "- Estimator의 하이퍼 파라미터를 튜닝하는 방법에는 여러 가지가 있습니다. 가장 일반적인 방법은 GridSearchCV() 함수를 사용하는 것입니다. GridSearchCV() 함수는 주어진 하이퍼 파라미터의 모든 조합에 대해 모델을 학습하고 평가하여 가장 좋은 하이퍼 파라미터를 찾습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5cc9ea86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3c34d077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 정확도: 1.0\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "train_data = iris.data\n",
    "train_label = iris.target\n",
    "dt_clf.fit(train_data, train_label)\n",
    "\n",
    "# 학습 데이터셋으로 예측 수행\n",
    "pred = dt_clf.predict(train_data)\n",
    "print('예측 정확도:', accuracy_score(train_label, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8a206194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 데이터셋 정확도: 0.9111\n"
     ]
    }
   ],
   "source": [
    "# 학습 및 검증 데이터를 7:3 으로 나누어 모델링 및 평가를 수행하세요\n",
    "\n",
    "iris = load_iris()\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=11)\n",
    "\n",
    "dt_clf.fit(X_train, y_train)\n",
    "\n",
    "# 테스트 데이터셋으로 예측 수행\n",
    "pred = dt_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, pred)\n",
    "print(\"테스트 데이터셋 정확도:\", round(accuracy, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06260d1f",
   "metadata": {},
   "source": [
    "### 교차 검증이 필요한 이유\n",
    "\n",
    "- 교차 유효성 검사는 독립적인 데이터 세트에서 모델의 성능을 평가하고 과적합을 방지하기 위해 기계 학습에 사용되는 통계 기법으로 다음과 같은 이유로 중요합니다.\n",
    "    - 과적합: 과적합은 모델이 노이즈나 이상값을 포함하여 훈련 데이터를 너무 잘 학습할 때 발생합니다. 이러한 경우 모델은 교육 데이터에 대해 뛰어난 정확도를 갖지만 보이지 않는 데이터에 대해서는 성능이 저하될 수 있습니다. 교차 검증은 보다 일반화된 성능 메트릭을 제공하여 과적합을 방지하는 데 도움이 됩니다.\n",
    "    - 하이퍼파라미터 조정: 교차 검증은 일반적으로 그리드 검색 또는 기타 하이퍼파라미터 튜닝 방법과 함께 사용되어 최상의 성능을 가진 모델을 생성하는 하이퍼파라미터를 찾습니다.\n",
    "    - 모델 비교: 여러 모델을 비교할 때 교차 검증은 서로 다른 데이터 하위 집합의 성능을 평균화하므로 보다 신뢰할 수 있는 비교를 제공하는 데 도움이 됩니다.\n",
    "\n",
    "- 교차 검증 방법 : k-겹 교차 검증\n",
    "    - 데이터세트 분할: 전체 데이터세트를 'k'개의 동일한 부분(또는 '접기')으로 나눕니다. k=5를 선택한다고 가정해 보겠습니다. 즉, 데이터를 5개의 하위 집합으로 나눕니다.\n",
    "    - 훈련 및 테스트: 하나의 하위 집합을 테스트 세트로 유지하고 나머지 k-1 하위 집합에서 모델을 훈련합니다.\n",
    "    - 평가: 테스트 세트에서 모델의 성능 메트릭(예: 문제에 따라 정확도, 정밀도, 재현율, F1 점수 등)을 계산합니다.\n",
    "    - 반복: 이 프로세스를 k번 반복합니다. 매번 다른 하위 집합을 테스트 세트로 사용하고 나머지 하위 집합을 훈련 세트로 사용합니다.\n",
    "    - 평균 성능: k 반복에 대한 성능 메트릭의 평균을 계산합니다. 이는 모델의 교차 유효성 검사 점수로, 성능에 대한 보다 일반화된 척도를 제공합니다.\n",
    "\n",
    "- 교차 유효성 검사의 기본 아이디어는 학습용 데이터에 대해 학습하고 검증용 데이터로 테스트할 때 얻은 \"점수\"가 새로운 데이터에서 모델이 수행되는 방식을 나타낼 가능성을 더 높다는 것입니다.\n",
    "- Python에서는 sklearn.model_selection 모듈의 cross_val_score 함수를 사용하여 교차 검증을 쉽게 수행할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7975847a",
   "metadata": {},
   "source": [
    "### 교차 검증 기술\n",
    "- 기계 학습 모델을 학습할 때 학습 데이터에서 학습된 패턴을 기반으로 새로운 보이지 않는 데이터에 대해 정확한 예측을 할 수 있는 모델을 만들려고 합니다. 문제는 모델을 실제로 테스트하기 전에는 본 적이 없는 데이터에서 모델이 얼마나 잘 작동할지 알 수 없다는 것입니다.\n",
    "- 교차 유효성 검사에는 전체 데이터 세트를 모델 훈련을 위한 더 큰 섹션과 모델 테스트를 위한 더 작은 섹션의 두 섹션으로 나누는 작업이 포함됩니다.\n",
    "- '트레이닝 세트'는 데이터의 기본 패턴에 대해 모델을 가르치는 데 사용됩니다. 반면 '테스트 세트'는 모델이 이러한 패턴을 얼마나 잘 학습했는지 평가하는 데 사용됩니다. 모델은 교육 중에 테스트 데이터를 본 적이 없으므로 테스트 세트는 '보이지 않는 새로운 데이터'로 작동합니다.\n",
    "- 이제 이 작은 부분(테스트 세트)에서 모델을 테스트할 때 얻은 \"점수\"는 모델이 미래에 실제 보이지 않는 데이터에서 얼마나 잘 수행될 것인지에 대한 추정치로 사용됩니다.\n",
    "- 서로 다른 데이터 하위 집합(교차 유효성 검사의 핵심)에 대해 이 프로세스를 여러 번 반복함으로써 새 데이터로 일반화하는 모델의 능력에 대해 보다 신뢰할 수 있는 추정치를 얻습니다.\n",
    "- 즉, 대부분의 데이터(훈련 세트)에서 모델을 훈련한 다음 데이터의 작은 부분(테스트 세트)에서 테스트함으로써 모델이 얼마나 잘 수행될 가능성이 있는지 보다 정확한 측정값을 얻을 수 있습니다. 완전히 새로운 데이터를 만나면 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "61791f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "붓꽃 데이터 세트 크기 150\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "iris = load_iris()\n",
    "features = iris.data\n",
    "label = iris.target\n",
    "dt_clf = DecisionTreeClassifier(random_state = 156)\n",
    "\n",
    "# 5개의 폴드 세트로 분리하는 KFold 객체와 풀드 세트별 정확도를 담을 리스트 객체 생성\n",
    "kfold = KFold(n_splits = 5)\n",
    "cv_accuracy = []\n",
    "print('붓꽃 데이터 세트 크기',features.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9fa44f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f24d61d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#1 교차 검증 정확도 : 1.0, 학습 데이터 크기: 120, 검층 데이터 크기: 30\n",
      "#1 검증 세트 인덱스: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29]\n",
      "\n",
      "#2 교차 검증 정확도 : 0.9667, 학습 데이터 크기: 120, 검층 데이터 크기: 30\n",
      "#2 검증 세트 인덱스: [30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53\n",
      " 54 55 56 57 58 59]\n",
      "\n",
      "#3 교차 검증 정확도 : 0.8667, 학습 데이터 크기: 120, 검층 데이터 크기: 30\n",
      "#3 검증 세트 인덱스: [60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83\n",
      " 84 85 86 87 88 89]\n",
      "\n",
      "#4 교차 검증 정확도 : 0.9333, 학습 데이터 크기: 120, 검층 데이터 크기: 30\n",
      "#4 검증 세트 인덱스: [ 90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119]\n",
      "\n",
      "#5 교차 검증 정확도 : 0.7333, 학습 데이터 크기: 120, 검층 데이터 크기: 30\n",
      "#5 검증 세트 인덱스: [120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137\n",
      " 138 139 140 141 142 143 144 145 146 147 148 149]\n",
      "\n",
      "## 평균 검증 정확도 :  0.9\n"
     ]
    }
   ],
   "source": [
    "n_iter = 0\n",
    "\n",
    "# KFold 객체의 split() 호출하면 폴드 별 학습용 검증용 테스트의 로우 인덱스를 array로 반환\n",
    "\n",
    "for train_index, test_index in kfold.split(features) :\n",
    "    # kfold.split() 으로 반환된 인덱스를 이용하여 학습용 검증용 테스트 데이터 추출\n",
    "    X_train, X_test = features[train_index], features[test_index]\n",
    "    y_train, y_test = label[train_index], label[test_index]\n",
    "    \n",
    "    # 학습 및 예측\n",
    "    dt_clf.fit(X_train, y_train)\n",
    "    pred = dt_clf.predict(X_test)\n",
    "    n_iter += 1\n",
    "    \n",
    "    # 반복시 마다 정확도 측정\n",
    "    \n",
    "    accuracy = np.round(accuracy_score(y_test, pred), 4)\n",
    "    train_size = X_train.shape[0]\n",
    "    test_size = X_test.shape[0]\n",
    "    print('\\n#{0} 교차 검증 정확도 : {1}, 학습 데이터 크기: {2}, 검층 데이터 크기: {3}'\n",
    "          .format(n_iter, accuracy, train_size, test_size))\n",
    "    print('#{0} 검증 세트 인덱스: {1}'.format(n_iter, test_index))\n",
    "    cv_accuracy.append(accuracy)\n",
    "\n",
    "# 개별 iteration 별 정확도를 합하여 평균 정확도 계산\n",
    "print('\\n## 평균 검증 정확도 : ', np.mean(cv_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb293c7",
   "metadata": {},
   "source": [
    "### 과제1 k값을 3, 10으로 설정하여 각각 교차 검증을  수행하고 검증 정확도가 가장 높은 k 값을 선택하세요\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "60840830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#1 교차 검증 정확도 : 0.0, 학습 데이터 크기: 100, 검층 데이터 크기: 50\n",
      "#1 검증 세트 인덱스: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49]\n",
      "\n",
      "#2 교차 검증 정확도 : 0.0, 학습 데이터 크기: 100, 검층 데이터 크기: 50\n",
      "#2 검증 세트 인덱스: [50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73\n",
      " 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97\n",
      " 98 99]\n",
      "\n",
      "#3 교차 검증 정확도 : 0.0, 학습 데이터 크기: 100, 검층 데이터 크기: 50\n",
      "#3 검증 세트 인덱스: [100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117\n",
      " 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135\n",
      " 136 137 138 139 140 141 142 143 144 145 146 147 148 149]\n",
      "\n",
      "## 평균 검증 정확도 :  0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "iris = load_iris()\n",
    "features = iris.data\n",
    "label = iris.target\n",
    "dt_clf = DecisionTreeClassifier(random_state = 156)\n",
    "\n",
    "kfold = KFold(n_splits = 3)\n",
    "cv_accuracy = []\n",
    "\n",
    "n_iter = 0\n",
    "\n",
    "for train_index, test_index in kfold.split(features) :\n",
    "    X_train, X_test = features[train_index], features[test_index]\n",
    "    y_train, y_test = label[train_index], label[test_index]\n",
    "    \n",
    "    dt_clf.fit(X_train, y_train)\n",
    "    pred = dt_clf.predict(X_test)\n",
    "    n_iter += 1\n",
    "    \n",
    "    accuracy = np.round(accuracy_score(y_test, pred), 4)\n",
    "    train_size = X_train.shape[0]\n",
    "    test_size = X_test.shape[0]\n",
    "    print('\\n#{0} 교차 검증 정확도 : {1}, 학습 데이터 크기: {2}, 검층 데이터 크기: {3}'\n",
    "          .format(n_iter, accuracy, train_size, test_size))\n",
    "    print('#{0} 검증 세트 인덱스: {1}'.format(n_iter, test_index))\n",
    "    cv_accuracy.append(accuracy)\n",
    "    \n",
    "print('\\n## 평균 검증 정확도 : ', np.mean(cv_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e585c07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#1 교차 검증 정확도 : 1.0, 학습 데이터 크기: 135, 검층 데이터 크기: 15\n",
      "#1 검증 세트 인덱스: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "\n",
      "#2 교차 검증 정확도 : 1.0, 학습 데이터 크기: 135, 검층 데이터 크기: 15\n",
      "#2 검증 세트 인덱스: [15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "\n",
      "#3 교차 검증 정확도 : 1.0, 학습 데이터 크기: 135, 검층 데이터 크기: 15\n",
      "#3 검증 세트 인덱스: [30 31 32 33 34 35 36 37 38 39 40 41 42 43 44]\n",
      "\n",
      "#4 교차 검증 정확도 : 0.9333, 학습 데이터 크기: 135, 검층 데이터 크기: 15\n",
      "#4 검증 세트 인덱스: [45 46 47 48 49 50 51 52 53 54 55 56 57 58 59]\n",
      "\n",
      "#5 교차 검증 정확도 : 0.9333, 학습 데이터 크기: 135, 검층 데이터 크기: 15\n",
      "#5 검증 세트 인덱스: [60 61 62 63 64 65 66 67 68 69 70 71 72 73 74]\n",
      "\n",
      "#6 교차 검증 정확도 : 0.8667, 학습 데이터 크기: 135, 검층 데이터 크기: 15\n",
      "#6 검증 세트 인덱스: [75 76 77 78 79 80 81 82 83 84 85 86 87 88 89]\n",
      "\n",
      "#7 교차 검증 정확도 : 1.0, 학습 데이터 크기: 135, 검층 데이터 크기: 15\n",
      "#7 검증 세트 인덱스: [ 90  91  92  93  94  95  96  97  98  99 100 101 102 103 104]\n",
      "\n",
      "#8 교차 검증 정확도 : 0.8667, 학습 데이터 크기: 135, 검층 데이터 크기: 15\n",
      "#8 검증 세트 인덱스: [105 106 107 108 109 110 111 112 113 114 115 116 117 118 119]\n",
      "\n",
      "#9 교차 검증 정확도 : 0.8, 학습 데이터 크기: 135, 검층 데이터 크기: 15\n",
      "#9 검증 세트 인덱스: [120 121 122 123 124 125 126 127 128 129 130 131 132 133 134]\n",
      "\n",
      "#10 교차 검증 정확도 : 1.0, 학습 데이터 크기: 135, 검층 데이터 크기: 15\n",
      "#10 검증 세트 인덱스: [135 136 137 138 139 140 141 142 143 144 145 146 147 148 149]\n",
      "\n",
      "## 평균 검증 정확도 :  0.9400000000000001\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "features = iris.data\n",
    "label = iris.target\n",
    "dt_clf = DecisionTreeClassifier(random_state = 156)\n",
    "\n",
    "kfold = KFold(n_splits = 10)\n",
    "cv_accuracy = []\n",
    "\n",
    "n_iter = 0\n",
    "\n",
    "for train_index, test_index in kfold.split(features) :\n",
    "    X_train, X_test = features[train_index], features[test_index]\n",
    "    y_train, y_test = label[train_index], label[test_index]\n",
    "    \n",
    "    dt_clf.fit(X_train, y_train)\n",
    "    pred = dt_clf.predict(X_test)\n",
    "    n_iter += 1\n",
    "    \n",
    "    accuracy = np.round(accuracy_score(y_test, pred), 4)\n",
    "    train_size = X_train.shape[0]\n",
    "    test_size = X_test.shape[0]\n",
    "    print('\\n#{0} 교차 검증 정확도 : {1}, 학습 데이터 크기: {2}, 검층 데이터 크기: {3}'\n",
    "          .format(n_iter, accuracy, train_size, test_size))\n",
    "    print('#{0} 검증 세트 인덱스: {1}'.format(n_iter, test_index))\n",
    "    cv_accuracy.append(accuracy)\n",
    "    \n",
    "print('\\n## 평균 검증 정확도 : ', np.mean(cv_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
